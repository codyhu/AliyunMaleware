{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_PATH = './feature/'\n",
    "DATA_PATH = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_vec = TfidfVectorizer(ngram_range=(1, 4),\n",
    "                          min_df=3,\n",
    "                          max_df=0.9,\n",
    "                          strip_accents='unicode',\n",
    "                          use_idf=1, smooth_idf=1, sublinear_tf=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData():\n",
    "    print(\"Loading original data ...\")\n",
    "    train_o_d = pd.read_csv(DATA_PATH+'security_train.csv')\n",
    "    test_o_d = pd.read_csv(DATA_PATH+'security_test.csv')\n",
    "    print(\"Loading completes\")\n",
    "    return train_o_d, test_o_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidfModelTrain(train, test):\n",
    "    tr_api = train.groupby('file_id')['api'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "    te_api = test.groupby('file_id')['api'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "    tr_api_vec = api_vec.fit_transform(tr_api['api'])\n",
    "    val_api_vec = api_vec.transform(te_api['api'])\n",
    "    return (tr_api_vec, val_api_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB-LR\n",
    "def pr(x, y_i, y):\n",
    "    p = x[y == y_i].sum(0)\n",
    "    return (p + 1) / ((y == y_i).sum() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mdl(x, y):\n",
    "    y = y.values\n",
    "    r = np.log(pr(x, 1, y) / pr(x, 0, y))\n",
    "    np.random.seed(0)\n",
    "    m = LogisticRegression(C=6, dual=True, random_state=0)\n",
    "    x_nb = x.multiply(r)\n",
    "    return m.fit(x_nb, y), r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nblrTrain(tr_tfidf_rlt, te_tfidf_rlt, train):\n",
    "    label_fold = []\n",
    "    preds_fold_lr = []\n",
    "    lr_oof = pd.DataFrame()\n",
    "    preds_te = []\n",
    "    skf = StratifiedKFold(n_splits=5, random_state=4, shuffle=True)\n",
    "    for fold_i, (tr_index, val_index) in enumerate(skf.split(train, train['label'])):\n",
    "        if fold_i >= 0:\n",
    "            tr, val = train.iloc[tr_index], train.iloc[val_index]\n",
    "            x = tr_tfidf_rlt[tr_index, :]\n",
    "            test_x = tr_tfidf_rlt[val_index, :]\n",
    "            preds = np.zeros((len(val), 1))\n",
    "            preds_te_i = np.zeros((te_tfidf_rlt.shape[0], 1))\n",
    "            labels = [i for i in range(1)]\n",
    "            for i, j in enumerate(labels):\n",
    "                print('fit', j)\n",
    "                m, r = get_mdl(x, tr['label'] == j)\n",
    "                preds[:, i] = m.predict_proba(test_x.multiply(r))[:, 1]\n",
    "                preds_te_i[:, i] = m.predict_proba(te_tfidf_rlt.multiply(r))[:, 1]\n",
    "            preds_te.append(preds_te_i)\n",
    "            preds_lr = preds\n",
    "            lr_oof_i = pd.DataFrame({'file_id': val['file_id']})\n",
    "            for i in range(1):\n",
    "                lr_oof_i['prob' + str(i)] = preds[:, i]\n",
    "            lr_oof = pd.concat([lr_oof, lr_oof_i], axis=0)\n",
    "\n",
    "            for i, j in enumerate(preds_lr):\n",
    "                preds_lr[i] = j / sum(j)\n",
    "\n",
    "            label_fold.append(val['label'].tolist())\n",
    "            preds_fold_lr.append(preds_lr)\n",
    "\n",
    "            lr_oof = lr_oof.sort_values('file_id')\n",
    "            preds_te_avg = (np.sum(np.array(preds_te), axis=0) / 5)\n",
    "            lr_oof_te = pd.DataFrame({'file_id': range(0, te_tfidf_rlt.shape[0])})\n",
    "            for i in range(1):\n",
    "                lr_oof_te['prob' + str(i)] = preds_te_avg[:, i]\n",
    "    return (lr_oof, lr_oof_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFeature(data, is_train=True):\n",
    "    '''\n",
    "    file_cnt: How many samples does file have\n",
    "    tid_distinct_cnt: How many threads the file launch\n",
    "    api_distinct_cnt: How many distinct numbers of APT called by file\n",
    "    tid_api_cnt_max,tid_api_cnt_min,tid_api_cnt_mean: the max/min/mean number of API called threads of file\n",
    "    tid_api_distinct_cnt_max, tid_api_distinct_cnt_min, tid_api_distinct_cnt_mean: the distinct max/min/mean number of API called threads of file\n",
    "    '''\n",
    "\n",
    "    if is_train:\n",
    "        return_data = data[['file_id', 'label']].drop_duplicates()\n",
    "    else:\n",
    "        return_data = data[['file_id']].drop_duplicates()\n",
    "\n",
    "    feat = data.groupby(['file_id']).tid.count().reset_index(name='file_cnt')\n",
    "    return_data = return_data.merge(feat, on='file_id', how='left')\n",
    "\n",
    "    ################################################################################\n",
    "    feat = data.groupby(['file_id']).agg({'tid': pd.Series.nunique, 'api': pd.Series.nunique}).reset_index()\n",
    "    feat.columns = ['file_id', 'tid_distinct_cnt', 'api_distinct_cnt']\n",
    "    return_data = return_data.merge(feat, on='file_id', how='left')\n",
    "    ################################################################################\n",
    "    feat_tmp = data.groupby(['file_id', 'tid']).agg({'index': pd.Series.count, 'api': pd.Series.nunique}).reset_index()\n",
    "    feat = feat_tmp.groupby(['file_id'])['index'].agg(['max', 'min', 'mean']).reset_index()\n",
    "    feat.columns = ['file_id', 'tid_api_cnt_max', 'tid_api_cnt_min', 'tid_api_cnt_mean']\n",
    "    return_data = return_data.merge(feat, on='file_id', how='left')\n",
    "\n",
    "    feat = feat_tmp.groupby(['file_id'])['api'].agg(['max', 'min', 'mean']).reset_index()\n",
    "    feat.columns = ['file_id', 'tid_api_distinct_cnt_max', 'tid_api_distinct_cnt_min', 'tid_api_distinct_cnt_mean']\n",
    "    return_data = return_data.merge(feat, on='file_id', how='left')\n",
    "\n",
    "    return return_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFeature_v2(data):\n",
    "    return_data = data[['file_id']].drop_duplicates()\n",
    "\n",
    "    # count the number of api called by file\n",
    "    tmp = data.groupby(['file_id']).api.count()\n",
    "\n",
    "    # caculate the min Index called by api\n",
    "    feat = data.groupby(['file_id', 'api'])['index'].min().reset_index(name='val')\n",
    "    feat = feat.pivot(index='file_id', columns='api', values='val')\n",
    "    feat.columns = [feat.columns[i] + '_index_min' for i in range(feat.shape[1])]\n",
    "    feat_withFileid = feat.reset_index()\n",
    "    return_data = return_data.merge(feat_withFileid, on='file_id', how='left')\n",
    "    #  count the number of called api\n",
    "    feat = data.groupby(['file_id', 'api'])['index'].count().reset_index(name='val')\n",
    "    feat = feat.pivot(index='file_id', columns='api', values='val')\n",
    "    feat.columns = [feat.columns[i] + '_cnt' for i in range(feat.shape[1])]\n",
    "    feat_withFileid = feat.reset_index()\n",
    "    return_data = return_data.merge(feat_withFileid, on='file_id', how='left')\n",
    "    #  caculate the proportion of api's calling\n",
    "    feat_rate = pd.concat([feat, tmp], axis=1)\n",
    "    feat_rate = feat_rate.apply(lambda x: x / feat_rate.api)\n",
    "    feat_rate.columns = [feat_rate.columns[i] + '_rate' for i in range(feat_rate.shape[1])]\n",
    "    feat_rate_withFileid = feat_rate.reset_index().drop(['api_rate'], axis=1)\n",
    "    return_data = return_data.merge(feat_rate_withFileid, on='file_id', how='left')\n",
    "\n",
    "    return return_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original data ...\n",
      "Loading completes\n"
     ]
    }
   ],
   "source": [
    " #load original data\n",
    "traindata, testdata = loadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Train Data:  (13887, 11)\n",
      "Base Train Data:  (13887, 886)\n"
     ]
    }
   ],
   "source": [
    "#make train data features\n",
    "train_base_feature_v1 = makeFeature(traindata, True)\n",
    "print('Base Train Data: ', train_base_feature_v1.shape)\n",
    "train_base_feature_v1.to_csv(FEATURE_PATH + 'train_base_features_v1.csv', index=None)\n",
    "\n",
    "train_base_feature_v2 = makeFeature_v2(traindata)\n",
    "print('Base Train Data: ', train_base_feature_v2.shape)\n",
    "train_base_feature_v2.to_csv(FEATURE_PATH + 'train_base_features_v2.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Test Data:  (12955, 10)\n",
      "Base Test Data:  (12955, 895)\n"
     ]
    }
   ],
   "source": [
    "# make test data features\n",
    "test_base_feature_v1 = makeFeature(testdata, False)\n",
    "print('Base Test Data: ', test_base_feature_v1.shape)\n",
    "test_base_feature_v1.to_csv(FEATURE_PATH + 'test_base_features_v1.csv', index=None)\n",
    "\n",
    "test_base_feature_v2 = makeFeature_v2(testdata)\n",
    "print('Base Test Data: ', test_base_feature_v2.shape)\n",
    "test_base_feature_v2.to_csv(FEATURE_PATH + 'test_base_features_v2.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make TFIDF and over_prob feature engineering\n",
    "tr_api_vec, val_api_vec = tfidfModelTrain(traindata, testdata)\n",
    "scipy.sparse.save_npz(FEATURE_PATH + 'tr_tfidf_rlt.npz', tr_api_vec)\n",
    "scipy.sparse.save_npz(FEATURE_PATH + 'te_tfidf_rlt.npz', val_api_vec)\n",
    "\n",
    "tr_prob, te_prob = nblrTrain(tr_api_vec, val_api_vec, train_base_feature_v1)\n",
    "tr_prob.to_csv(FEATURE_PATH + 'tr_lr_oof_prob.csv', index=False)\n",
    "te_prob.to_csv(FEATURE_PATH + 'te_lr_oof_prob.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
